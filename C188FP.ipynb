{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eda318a7-15fc-409d-94b0-6235d246dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import chess\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gym import spaces\n",
    "from collections import deque\n",
    "import math\n",
    "import sys\n",
    "\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import gym\n",
    "# import chess\n",
    "# from gym import spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab67a37f-0038-4767-a893-1d5f84871411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fen_positions(csv_file, balanced_only=True):\n",
    "    fen_list = []\n",
    "    with open(csv_file, 'r', newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)  \n",
    "        for row in reader:\n",
    "            fen = row['fen']\n",
    "            board = chess.Board(fen)\n",
    "            if not balanced_only or is_material_balanced(board):\n",
    "                fen_list.append(fen)\n",
    "    return fen_list\n",
    "\n",
    "def is_material_balanced(board):\n",
    "    piece_values = {chess.PAWN:1, chess.KNIGHT:3, chess.BISHOP:3, chess.ROOK:5, chess.QUEEN:9}\n",
    "    balance = 0\n",
    "    for sq in chess.SQUARES:\n",
    "        piece = board.piece_at(sq)\n",
    "        if piece:\n",
    "            sign = 1 if piece.color == chess.WHITE else -1\n",
    "            balance += sign * piece_values.get(piece.piece_type, 0)\n",
    "    return abs(balance) <= 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70469dac-6f31-4a56-bf34-77d75b0a2c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chess Environment (Gym)\n",
    "class ChessEnv(gym.Env):\n",
    "    def __init__(self, max_steps=100):\n",
    "        super().__init__()\n",
    "        self.board = chess.Board()\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step_count = 0\n",
    "        self.observation_space = spaces.Box(low=-6, high=6, shape=(64,), dtype=np.int8)\n",
    "        self.action_space = spaces.Discrete(64 * 64)\n",
    "\n",
    "    def reset(self):\n",
    "        self.board.reset()\n",
    "        self.current_step_count = 0\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step_count += 1\n",
    "        move = self._decode_action(action)\n",
    "        if move in self.board.legal_moves:\n",
    "            self.board.push(move)\n",
    "        else:\n",
    "            legal = list(self.board.legal_moves)\n",
    "            if legal:\n",
    "                self.board.push(random.choice(legal))\n",
    "        done = self.board.is_game_over() or (self.current_step_count >= self.max_steps)\n",
    "        reward = self._get_reward(done)\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def _get_reward(self, done):\n",
    "        if not done:\n",
    "            return 0.0\n",
    "        if self.board.is_checkmate():\n",
    "            return -1.0\n",
    "        return 0.0\n",
    "\n",
    "    def _get_observation(self):\n",
    "        obs = np.zeros(64, dtype=np.int8)\n",
    "        for i in range(64):\n",
    "            piece = self.board.piece_at(i)\n",
    "            if piece:\n",
    "                val = piece.piece_type if piece.color == chess.WHITE else -piece.piece_type\n",
    "                obs[i] = val\n",
    "        return obs\n",
    "\n",
    "    def _decode_action(self, action_idx):\n",
    "        from_sq = action_idx // 64\n",
    "        to_sq   = action_idx % 64\n",
    "        return chess.Move(from_sq, to_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d64eaef7-2358-4824-a652-a5be9314d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FENDatasetChessEnv(gym.Env):\n",
    "    def __init__(self, fen_list, max_steps=100):\n",
    "        super().__init__()\n",
    "        self.fen_list = fen_list\n",
    "        self.max_steps = max_steps\n",
    "        self.board = chess.Board()\n",
    "        self.current_step_count = 0\n",
    "\n",
    "        self.observation_space = spaces.Box(low=-6, high=6, shape=(64,), dtype=np.int8)\n",
    "        self.action_space = spaces.Discrete(64 * 64)\n",
    "\n",
    "    def reset(self):\n",
    "        # Pick one random FEN each episode\n",
    "        fen = random.choice(self.fen_list)\n",
    "        self.board.set_fen(fen)\n",
    "        self.current_step_count = 0\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step_count += 1\n",
    "        move = self._decode_action(action)\n",
    "        if move in self.board.legal_moves:\n",
    "            self.board.push(move)\n",
    "        else:\n",
    "            legal = list(self.board.legal_moves)\n",
    "            if legal:\n",
    "                self.board.push(random.choice(legal))\n",
    "        done = self.board.is_game_over() or (self.current_step_count >= self.max_steps)\n",
    "        reward = self._get_reward(done)\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def _get_reward(self, done):\n",
    "        if not done:\n",
    "            return 0.0\n",
    "        if self.board.is_checkmate():\n",
    "            return -1.0\n",
    "        return 0.0\n",
    "\n",
    "    def _get_observation(self):\n",
    "        obs = np.zeros(64, dtype=np.int8)\n",
    "        for i in range(64):\n",
    "            piece = self.board.piece_at(i)\n",
    "            if piece:\n",
    "                val = piece.piece_type if piece.color else -piece.piece_type\n",
    "                obs[i] = val\n",
    "        return obs\n",
    "\n",
    "    def _decode_action(self, action_idx):\n",
    "        from_sq = action_idx // 64\n",
    "        to_sq   = action_idx % 64\n",
    "        return chess.Move(from_sq, to_sq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "158d4b67-f9aa-40bd-b38f-d7357a88d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-Learning\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.q_table = {}\n",
    "\n",
    "    def get_q(self, state_key, action):\n",
    "        return self.q_table.get((state_key, action), 0.0)\n",
    "\n",
    "    def set_q(self, state_key, action, value):\n",
    "        self.q_table[(state_key, action)] = value\n",
    "\n",
    "    def choose_action(self, state, env):\n",
    "        state_key = self._state_to_key(state)\n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            legal_moves = list(env.board.legal_moves)\n",
    "            if not legal_moves:\n",
    "                return env.action_space.sample()\n",
    "                \n",
    "            best_action = None\n",
    "            best_q_val = -float('inf')\n",
    "            \n",
    "            for move in legal_moves:\n",
    "                action_idx = self._encode_action(move)\n",
    "                q_val = self.get_q(state_key, action_idx)\n",
    "                if q_val > best_q_val:\n",
    "                    best_q_val = q_val\n",
    "                    best_action = action_idx\n",
    "            \n",
    "            return best_action if best_action is not None else env.action_space.sample()\n",
    "\n",
    "    def update(self, old_state, action, reward, new_state, done, env):\n",
    "        old_key = self._state_to_key(old_state)\n",
    "        new_key = self._state_to_key(new_state)\n",
    "\n",
    "        old_q = self.get_q(old_key, action)\n",
    "        \n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            best_next_q = -float('inf')\n",
    "            for m in env.board.legal_moves:\n",
    "                next_a = self._encode_action(m)\n",
    "                best_next_q = max(best_next_q, self.get_q(new_key, next_a))\n",
    "            td_target = reward + self.gamma * best_next_q\n",
    "\n",
    "        updated_q = old_q + self.alpha * (td_target - old_q)\n",
    "        self.set_q(old_key, action, updated_q)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def _state_to_key(self, state):\n",
    "        return tuple(state.tolist())\n",
    "\n",
    "    def _encode_action(self, move):\n",
    "        return move.from_square * 64 + move.to_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8e75294-32a7-4126-bdfa-1caa30ffcbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing SARSA\n",
    "class SARSAAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.q_table = {}\n",
    "\n",
    "    def get_q(self, state_key, action):\n",
    "        return self.q_table.get((state_key, action), 0.0)\n",
    "\n",
    "    def set_q(self, state_key, action, value):\n",
    "        self.q_table[(state_key, action)] = value\n",
    "\n",
    "    def choose_action(self, state, env):\n",
    "        state_key = self._state_to_key(state)\n",
    "        if random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            legal_moves = list(env.board.legal_moves)\n",
    "            if not legal_moves:\n",
    "                return env.action_space.sample()\n",
    "            \n",
    "            best_action = None\n",
    "            best_q_val = -float('inf')\n",
    "            for move in legal_moves:\n",
    "                a_idx = self._encode_action(move)\n",
    "                q_val = self.get_q(state_key, a_idx)\n",
    "                if q_val > best_q_val:\n",
    "                    best_q_val = q_val\n",
    "                    best_action = a_idx\n",
    "            return best_action if best_action is not None else env.action_space.sample()\n",
    "\n",
    "    def update(self, old_state, action, reward, new_state, new_action, done):\n",
    "\n",
    "        old_key = self._state_to_key(old_state)\n",
    "        new_key = self._state_to_key(new_state)\n",
    "\n",
    "        old_q = self.get_q(old_key, action)\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            new_q = self.get_q(new_key, new_action)\n",
    "            td_target = reward + self.gamma * new_q\n",
    "\n",
    "        updated_q = old_q + self.alpha * (td_target - old_q)\n",
    "        self.set_q(old_key, action, updated_q)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def _state_to_key(self, state):\n",
    "        return tuple(state.tolist())\n",
    "\n",
    "    def _encode_action(self, move):\n",
    "        return move.from_square * 64 + move.to_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f131f0a6-d3a7-4078-8541-9bc9e76e8af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing MCTS \n",
    "class MCTSAgent:\n",
    "    def __init__(self, n_simulations=100, c_puct=1.4):\n",
    "        self.n_simulations = n_simulations\n",
    "        self.c_puct = c_puct\n",
    "        self.tree = {}  \n",
    "\n",
    "    def choose_action(self, env):\n",
    "        state_fen = env.board.fen()\n",
    "        \n",
    "        for _ in range(self.n_simulations):\n",
    "            self._simulate(env.board.copy())\n",
    "        \n",
    "        root_node = self.tree.get(state_fen, None)\n",
    "        if root_node is None or not root_node[\"children\"]:\n",
    "            legal_moves = list(env.board.legal_moves)\n",
    "            if not legal_moves:\n",
    "                return env.action_space.sample()\n",
    "            return self._encode_action(random.choice(legal_moves))\n",
    "        \n",
    "        best_move, best_stats = None, None\n",
    "        for move, child_fen in root_node[\"children\"].items():\n",
    "            child_node = self.tree.get(child_fen, None)\n",
    "            if child_node is None:\n",
    "                continue\n",
    "            if best_stats is None or child_node[\"N\"] > best_stats[\"N\"]:\n",
    "                best_move = move\n",
    "                best_stats = child_node\n",
    "        \n",
    "        if best_move is None:\n",
    "            legal_moves = list(env.board.legal_moves)\n",
    "            if not legal_moves:\n",
    "                return env.action_space.sample()\n",
    "            return self._encode_action(random.choice(legal_moves))\n",
    "        \n",
    "        return self._encode_action(best_move)\n",
    "\n",
    "    def _simulate(self, board):\n",
    "        visited = []\n",
    "        fen_history = []\n",
    "        \n",
    "        while True:\n",
    "            fen = board.fen()\n",
    "            node = self.tree.setdefault(fen, {\"N\": 0, \"W\": 0.0, \"children\": {}})\n",
    "            visited.append(fen)\n",
    "\n",
    "            if board.is_game_over():\n",
    "                break\n",
    "\n",
    "            legal_moves = list(board.legal_moves)\n",
    "            if not legal_moves:\n",
    "                break\n",
    "\n",
    "            if not node[\"children\"]:\n",
    "                for move in legal_moves:\n",
    "                    board.push(move)\n",
    "                    child_fen = board.fen()\n",
    "                    node[\"children\"][move] = child_fen\n",
    "                    board.pop()\n",
    "\n",
    "            move = self._select_child(node, board)\n",
    "            board.push(move)\n",
    "            fen_history.append((fen, move))\n",
    "\n",
    "        reward = self._get_terminal_reward(board)\n",
    "        \n",
    "        for fen in visited:\n",
    "            node = self.tree[fen]\n",
    "            node[\"N\"] += 1\n",
    "            node[\"W\"] += reward\n",
    "        return\n",
    "\n",
    "    def _select_child(self, node, board):\n",
    "        best_score = -float('inf')\n",
    "        best_move = None\n",
    "        \n",
    "        N_parent = node[\"N\"] + 1e-8\n",
    "        for move, child_fen in node[\"children\"].items():\n",
    "            child_node = self.tree.setdefault(child_fen, {\"N\": 0, \"W\": 0.0, \"children\": {}})\n",
    "            \n",
    "            if child_node[\"N\"] == 0:\n",
    "                q_value = 0.0\n",
    "            else:\n",
    "                q_value = child_node[\"W\"] / child_node[\"N\"]\n",
    "                \n",
    "            ucb = q_value + self.c_puct * np.sqrt(np.log(N_parent) / (child_node[\"N\"] + 1e-8))\n",
    "            if ucb > best_score:\n",
    "                best_score = ucb\n",
    "                best_move = move\n",
    "        return best_move\n",
    "\n",
    "    def _get_terminal_reward(self, board):\n",
    "        if board.is_checkmate():\n",
    "            return -1.0\n",
    "        return 0.0\n",
    "\n",
    "    def _encode_action(self, move):\n",
    "        return move.from_square * 64 + move.to_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "656c6eec-4d44-4010-8477-f90c077beee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def run_episodes(env, agent, num_episodes=1000, method=\"q_learning\"):\n",
    "    rewards_history = []\n",
    "    for ep in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "\n",
    "        if method == \"sarsa\":\n",
    "            action = agent.choose_action(obs, env)\n",
    "\n",
    "        while not done:\n",
    "            if method == \"q_learning\":\n",
    "                action = agent.choose_action(obs, env)\n",
    "                next_obs, reward, done, _ = env.step(action)\n",
    "                agent.update(obs, action, reward, next_obs, done, env)\n",
    "                obs = next_obs\n",
    "                total_reward += reward\n",
    "            elif method == \"sarsa\":\n",
    "                next_obs, reward, done, _ = env.step(action)\n",
    "                if not done:\n",
    "                    next_action = agent.choose_action(next_obs, env)\n",
    "                else:\n",
    "                    next_action = None\n",
    "                agent.update(obs, action, reward, next_obs, next_action, done)\n",
    "                obs = next_obs\n",
    "                action = next_action\n",
    "                total_reward += reward\n",
    "            else:\n",
    "                raise ValueError(\"Unknown method for run_episodes\")\n",
    "\n",
    "        rewards_history.append(total_reward)\n",
    "        if (ep+1) % 100 == 0:\n",
    "            print(f\"Episode {ep+1}/{num_episodes}, Reward={total_reward:.2f}\")\n",
    "    return rewards_history\n",
    "\n",
    "def run_episodes_mcts(env, agent, num_episodes=100):\n",
    "    rewards_history = []\n",
    "    for ep in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        while not done:\n",
    "            action_idx = agent.choose_action(env)\n",
    "            next_obs, reward, done, _ = env.step(action_idx)\n",
    "            total_reward += reward\n",
    "        rewards_history.append(total_reward)\n",
    "        if (ep+1) % 10 == 0:\n",
    "            print(f\"MCTS Episode {ep+1}/{num_episodes}, Reward={total_reward:.2f}\")\n",
    "    return rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e061a502-d0b8-4486-9593-39161c63f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(method=\"q_learning\", use_dataset=False, csv_file=None):\n",
    "    # python final_chess.py q_learning\n",
    "    # python final_chess.py sarsa\n",
    "    # python final_chess.py mcts\n",
    "\n",
    "    if method not in [\"q_learning\", \"sarsa\", \"mcts\"]:\n",
    "        print(\"Usage: python final_chess.py [q_learning|sarsa|mcts]\")\n",
    "        return\n",
    "\n",
    "    if len(sys.argv) >= 3 and sys.argv[2].lower() == \"dataset\":\n",
    "        use_dataset = True\n",
    "        if len(sys.argv) < 4:\n",
    "            print(\"Please provide a CSV file after 'dataset'!\")\n",
    "            sys.exit(1)\n",
    "        csv_file = sys.argv[3]\n",
    "        print(f\"Loading dataset from {csv_file}...\")\n",
    "        fen_dataset = load_fen_positions(csv_file, balanced_only=True)\n",
    "        print(f\"Loaded {len(fen_dataset)} FENs from dataset.\")\n",
    "\n",
    "    if use_dataset:\n",
    "        env = FENDatasetChessEnv(fen_dataset, max_steps=50)\n",
    "    else:\n",
    "        env = ChessEnv(max_steps=50)\n",
    "\n",
    "    if method == \"q_learning\":\n",
    "        agent = QLearningAgent(alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995)\n",
    "        run_episodes(env, agent, num_episodes=500, method=\"q_learning\")\n",
    "    \n",
    "    elif method == \"sarsa\":\n",
    "        agent = SARSAAgent(alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995)\n",
    "        run_episodes(env, agent, num_episodes=500, method=\"sarsa\")\n",
    "    \n",
    "    elif method == \"mcts\":\n",
    "        agent = MCTSAgent(n_simulations=100, c_puct=1.4)\n",
    "        run_episodes_mcts(env, agent, num_episodes=50)\n",
    "    \n",
    "    else:\n",
    "        print(\"Invalid argument. Choose from [q_learning|sarsa|mcts].\")\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be57e2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
